
# ğŸ“˜ Lab 01 â€” Complete Solution (Parts 1â€“3)

This repository contains a full solution to **Lab Assignment #1** from the Machine Learning course.  
The lab consists of three major parts:

- **Part 1:** Matrix calculus & kNN
- **Part 2:** ML pipeline â€” preprocessing, PCA, logistic regression, ensembling
- **Part 3:** SVM & kernels on synthetic datasets

All solutions include:

- reproducible code
- clean visualizations
- detailed explanations
- final conclusions for each part

---

# ğŸ§® Part 1 â€” Matrix Calculus & kNN

### âœ” Topics:

- vector & matrix differentiation
- trace trick
- Frobenius norm gradients
- matrix factorization gradient
- L1- and L2-based nearest neighbors
- distance matrix interpretation
- classification behavior of k-NN

### âœ” Key Results:

- all gradients (âˆ‚J/âˆ‚A, âˆ‚J/âˆ‚S) derived correctly
- 1-NN behavior explained via distance-matrix patterns
- effect of data preprocessing on L1 kNN analyzed
- theoretical questions answered cleanly and concisely

---

# ğŸ”§ Part 2 â€” ML Pipeline: Preprocessing, PCA, Models

### âœ” Workflow implemented:

- train/test split
- feature scaling
- PCA: explained variance â†’ dimension reduction
- logistic regression (raw & PCA-transformed)
- decision tree with CV
- bagging (LR & DT ensembles)
- random forest
- XGBoost (native API, no sklearn wrapper)
- learning curves

### âœ” Highlights:

- tuned PCA (â‰ˆ95% variance â†’ 8 components)
- multinomial LR + confusion matrix
- DT optimal depth via CV
- bagging improves stability
- RF & boosting outperform single models
- full analysis of how ensemble size affects metrics
- learning curves demonstrate bias/variance behavior

---

# ğŸ¯ Part 3 â€” SVM & Kernels

### âœ” Experiments:

- linear LR vs linear SVM
- SVM with polynomial, RBF, sigmoid kernels
- visualization of decision regions
- comparison of nonlinear boundaries
- PolynomialFeatures + Logistic Regression
- complex dataset (circles + moons, 4 classes)
- refactored grid search for LR+Poly and SVM RBF
- explicit vs implicit polynomial mapping explained

### âœ” Best Models:

- **RBF SVM:** 0.99 accuracy on moons
- **LR + Poly (deg=4):** 0.94 accuracy on complex dataset
- **RBF SVM:** 0.95 on complex dataset (best overall)

---

# ğŸ“ Final Conclusions

### âœ” Part 1

Matrix calculus builds the foundation for gradient-based ML methods.  
k-NN behavior strongly depends on geometry of the dataset and distance preprocessing.

### âœ” Part 2

PCA dramatically simplifies the feature space without major loss of information.  
Logistic regression is strong on well-scaled data; tree-based models capture nonlinearities.  
Bagging, Random Forest, and Boosting consistently outperform single models.

### âœ” Part 3

Linear models fail on nonlinear geometry, but both **Kernel SVM** and **PolynomialFeatures+LR**  
solve the moons and circles datasets extremely well.  
RBF kernels remain the most flexible and robust across nonlinear structures.
